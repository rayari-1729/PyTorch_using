{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc5ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c675fb",
   "metadata": {},
   "source": [
    "Pytorch is a python package with enables users to train state-of-the art machine learning/deep learning models.In order to efficiently use pytorch, one needs to have a firm underdtanding of the basic building blocks of pytorch: the torch.tensor object.In many ways , it's similar numpy array "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf2566",
   "metadata": {},
   "source": [
    "# Numpy vs. Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598f5e7",
   "metadata": {},
   "source": [
    "Numpy array's aand pytorch tensors can be created in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6861239",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(0,1,5)\n",
    "t = torch.linspace(0,1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d915c1",
   "metadata": {},
   "source": [
    "They can be resized in similar ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f8375a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a99db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.arange(48).reshape(3,4,4)\n",
    "t = torch.arange(48).reshape(3,4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59409a03",
   "metadata": {},
   "source": [
    "# General Broadcasting Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c2bb82",
   "metadata": {},
   "source": [
    "When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when\n",
    "   1. they are equal, or\n",
    "   2. one of them is 1\n",
    "  Example: The following are compatible\n",
    "\n",
    "  Shape 1: (1,6,4,1,7,2)\n",
    "\n",
    "  Shape 2: (5,6,1,3,1,2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d91a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2])\n",
    "b = np.array([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e5382a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]] \n",
      " [[0 1 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((6,5))\n",
    "b = np.arange(5).reshape((1,5))\n",
    "print(a,\"\\n\",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cf63329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45b29b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((6,5))\n",
    "b = torch.arange(5).reshape((1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b9a8e",
   "metadata": {},
   "source": [
    "The arrays/tensors don't need to have the same number of dimenions. If one of the arrays/tensors has less dimensions than the other\n",
    "\n",
    "**Example**: Scaling each other the color channels of an image by a different amount:\n",
    "\n",
    "     Image  (3d array): 256 x 256 x 3   \n",
    "     Scale  (1d array):             3\n",
    "     Result (3d array): 256 x 256 x 3\n",
    "     \n",
    "(3 values to each of these pixels red , blue and green so each pixels have three                                                 different values to scale for the colors )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a82c1422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000, 1.5000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "Scale = torch.tensor([0.5,1.5,1]) \n",
    "print(Scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f77f560",
   "metadata": {},
   "source": [
    "all the pixels in my array i have red multiply by 1/2 , blue multiply ny 1.5 and keep green same . \n",
    "Now Scale is one dim array where Image is 3d array . Multiplication of these 2 array is happening like this \n",
    "\n",
    "if Image =\n",
    "           \n",
    "           [[[ 0.2, 1.2 , 0.4 ],\n",
    "             [0.1, 0.19 , -0.7].\n",
    "            [[ 0.9, 0.9 , 2.0 ]]]\n",
    "Then    Image*Scale will be like [0.2 * 0.5 , 1.2* 1.5 , 0.4 * 1 ]    goes on .      \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee87e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image = torch.randn((256,256,3))\n",
    "Scale = torch.tensor([0.5,1.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebacef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = Image*Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2849c9f1",
   "metadata": {},
   "source": [
    "**Example**: One has an array of **2 images** and wants to scale the color channels of each image by a slightly different amount:\n",
    "\n",
    "    Images  (4d array): 2 x 256 x 256 x 3\n",
    "    Scales  (4d array): 2 x 1 x 1 x 3\n",
    "    Results  (4d array): 2 x 256 x 256 x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6195ca",
   "metadata": {},
   "source": [
    "we have 2 different pictures so hence 2 is there in first in Images whcih is called batch dimension in pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6b4ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = torch.randn((2,256,256,3))\n",
    "Scales = torch.tensor([0.5,1.5,1,1.5,1,0.5]).reshape((2,1,1,3)) ## reshape is very important here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14caca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Images * Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70598f85",
   "metadata": {},
   "source": [
    "# Operations Across Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f21ccca",
   "metadata": {},
   "source": [
    "This is so fundamental for pytorch. Obviously simple operations can be done one 1 dimensional tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27670330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1250), tensor(1.6520), tensor(4.), tensor(0.5000))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([0.5,1,3,4])\n",
    "torch.mean(t), torch.std(t), torch.max(t), torch.min(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450e40d",
   "metadata": {},
   "source": [
    "But suppose we have a 2d tensor, for example, and want to compute the mean value of each columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b5d2a",
   "metadata": {},
   "source": [
    "* Note: taking the mean of each column means taking the mean across the rows (which are the first dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9ce317c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(20, dtype=float).reshape(5,4)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7943a2",
   "metadata": {},
   "source": [
    "We want to mean across the row to get the mean each col and here we can see that 8 will be mean of 1st col and 9 will be mean of 2nd col so on ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d029b75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  9., 10., 11.], dtype=torch.float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(20, dtype=float).reshape(5,4)\n",
    "torch.mean(t, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019319b7",
   "metadata": {},
   "source": [
    "This can be done for higher dimensionality arrays as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "257d3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(5,256,256,3) ##5 images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae13d3",
   "metadata": {},
   "source": [
    "Take the mean across the batch (size 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1eec1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.7482e-01,  3.5627e-01,  2.0569e-01],\n",
       "         [ 1.5947e-01, -1.8948e-01, -1.7222e-01],\n",
       "         [-5.5312e-02, -1.4366e-01,  1.0664e-01],\n",
       "         ...,\n",
       "         [-9.6756e-02,  6.2063e-01,  1.8628e-01],\n",
       "         [-1.3244e-01,  1.3641e-01, -3.8420e-01],\n",
       "         [-2.1160e-01, -4.8154e-01,  1.5751e-01]],\n",
       "\n",
       "        [[ 3.8579e-01,  4.9652e-01, -1.0070e-01],\n",
       "         [-6.4069e-01,  7.5703e-01,  2.4229e-01],\n",
       "         [-5.6776e-01, -6.6937e-01, -2.8263e-01],\n",
       "         ...,\n",
       "         [-5.2531e-01,  3.9014e-01,  3.6667e-01],\n",
       "         [ 6.6242e-01, -2.9571e-02, -3.8872e-01],\n",
       "         [-5.6407e-01,  5.9965e-01, -5.0289e-01]],\n",
       "\n",
       "        [[-2.9267e-01, -7.8050e-02,  4.3210e-01],\n",
       "         [ 4.9271e-01,  3.4681e-01, -3.8555e-01],\n",
       "         [ 3.9004e-01, -4.3060e-02,  2.8223e-02],\n",
       "         ...,\n",
       "         [-2.3729e-01, -2.5815e-01, -7.9041e-01],\n",
       "         [-1.2855e-02,  3.3961e-01, -1.9748e-01],\n",
       "         [-6.6381e-01,  3.2402e-01,  8.7232e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-6.8224e-02, -9.0065e-01, -2.8068e-01],\n",
       "         [ 3.4549e-01, -3.0361e-02, -3.5826e-01],\n",
       "         [ 3.1845e-01,  4.2386e-02, -1.5248e-02],\n",
       "         ...,\n",
       "         [ 5.5709e-01,  5.5553e-04,  2.0060e-01],\n",
       "         [ 9.0565e-01, -1.2002e-01,  6.9727e-02],\n",
       "         [-1.3136e-01,  7.5668e-01,  6.3656e-01]],\n",
       "\n",
       "        [[ 4.2062e-01, -4.7070e-01, -6.7960e-01],\n",
       "         [ 2.9518e-01, -1.1795e-01,  7.0675e-01],\n",
       "         [-7.3655e-01,  1.2280e-02, -2.9349e-01],\n",
       "         ...,\n",
       "         [-4.1905e-01,  1.7658e-01, -2.5861e-02],\n",
       "         [-1.2582e-01, -6.0408e-01,  2.2708e-01],\n",
       "         [ 2.4453e-01, -2.4409e-01, -1.9668e-02]],\n",
       "\n",
       "        [[-4.0553e-01, -2.4706e-01, -3.8365e-01],\n",
       "         [ 1.9434e-01, -3.0663e-01,  6.8453e-01],\n",
       "         [-2.2478e-01, -4.8852e-01, -1.6668e-01],\n",
       "         ...,\n",
       "         [ 6.1376e-01,  5.6400e-01,  6.4634e-01],\n",
       "         [ 2.5683e-01, -3.7561e-01,  1.6353e-01],\n",
       "         [ 6.0302e-01, -5.3545e-01,  3.4837e-01]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t,axis=0) ## Operation across the first axis here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5979521",
   "metadata": {},
   "source": [
    "Take the mean across the color channels: \n",
    "want to average of the red blue and green power this might give a measure of brightness of each pixels that can convert to gray scale . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bb2f6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4106e+00, -2.7615e-01,  8.5342e-02,  ...,  7.2892e-02,\n",
       "          -2.8161e-01,  2.6944e-01],\n",
       "         [-3.5202e-01,  3.8313e-02, -6.5711e-01,  ..., -1.3036e-02,\n",
       "           9.9509e-02, -2.8339e-01],\n",
       "         [ 4.1021e-01,  3.5506e-01,  1.3502e-01,  ..., -5.5194e-01,\n",
       "           5.0868e-01, -1.8840e-01],\n",
       "         ...,\n",
       "         [-1.6149e-01,  4.8096e-01, -4.3422e-01,  ...,  2.6981e-01,\n",
       "           8.2800e-01,  7.1447e-01],\n",
       "         [ 4.9447e-01, -9.8311e-02,  7.2643e-01,  ...,  2.5160e-01,\n",
       "           2.0414e-01, -1.7318e-01],\n",
       "         [ 9.9739e-01, -1.7489e-01, -6.6753e-01,  ...,  1.0314e+00,\n",
       "           2.6161e-02,  1.2152e+00]],\n",
       "\n",
       "        [[-2.4848e-01,  1.5002e-01, -5.2520e-01,  ...,  2.3190e-01,\n",
       "          -4.1808e-01, -5.0253e-02],\n",
       "         [ 7.8123e-01, -2.6072e-01,  1.5602e-02,  ...,  4.1701e-01,\n",
       "          -5.3841e-01,  1.9789e-01],\n",
       "         [ 1.7592e-01, -5.1292e-01,  1.9638e-01,  ..., -4.0130e-01,\n",
       "          -1.0494e+00,  4.3987e-01],\n",
       "         ...,\n",
       "         [-9.2025e-01,  1.9648e-02,  4.8501e-01,  ...,  7.9862e-01,\n",
       "           5.1873e-01, -2.2526e-02],\n",
       "         [-1.3654e+00,  6.1495e-01, -3.1871e-01,  ..., -7.8778e-02,\n",
       "          -5.8017e-01, -8.3431e-02],\n",
       "         [-1.0061e+00,  1.5617e+00, -5.4274e-02,  ...,  4.5177e-01,\n",
       "           6.6703e-01,  3.1325e-01]],\n",
       "\n",
       "        [[ 3.0178e-01, -2.4955e-01,  7.8646e-01,  ...,  2.4827e-01,\n",
       "           9.2589e-01, -3.9080e-01],\n",
       "         [-2.3141e-01,  8.6596e-02, -1.3502e+00,  ..., -3.1581e-01,\n",
       "           1.0656e-01, -4.6671e-01],\n",
       "         [ 9.2213e-05,  1.1425e+00, -4.3739e-02,  ..., -6.8088e-01,\n",
       "           1.2289e+00, -5.1291e-02],\n",
       "         ...,\n",
       "         [ 3.4107e-01, -3.6017e-01,  5.9225e-01,  ...,  2.3737e-01,\n",
       "          -1.8430e-01,  6.0194e-01],\n",
       "         [ 8.5846e-01,  3.9394e-01, -9.2666e-01,  ..., -8.7865e-02,\n",
       "          -5.9504e-01,  5.0074e-01],\n",
       "         [ 3.0134e-01,  7.8459e-02, -4.0637e-01,  ..., -7.3142e-02,\n",
       "          -6.9260e-01,  3.3186e-01]],\n",
       "\n",
       "        [[-4.6507e-01,  3.3078e-01, -3.9407e-01,  ...,  4.8979e-01,\n",
       "           2.5768e-01, -8.5461e-01],\n",
       "         [ 7.4804e-01,  3.3304e-01, -6.6509e-01,  ...,  1.6095e-01,\n",
       "           1.0235e-01, -1.6298e-01],\n",
       "         [ 3.7165e-01,  1.5882e-01, -4.0044e-01,  ...,  3.0127e-01,\n",
       "           1.1620e-01,  8.0632e-01],\n",
       "         ...,\n",
       "         [-9.5359e-01,  2.1442e-01, -5.9946e-01,  ..., -2.2190e-01,\n",
       "          -1.5622e-01,  3.0363e-01],\n",
       "         [-1.1774e-01, -3.8711e-01, -6.1332e-01,  ...,  1.2304e-01,\n",
       "          -1.8314e-01,  2.4279e-01],\n",
       "         [-6.0883e-01,  5.8654e-02, -1.9625e-01,  ...,  3.9762e-01,\n",
       "           1.4379e-01, -3.6501e-01]],\n",
       "\n",
       "        [[-5.2033e-01, -2.9216e-01, -1.0642e-01,  ...,  1.4072e-01,\n",
       "          -1.1176e+00,  1.3350e-01],\n",
       "         [ 3.5684e-01,  4.0050e-01,  1.2387e-01,  ...,  1.3673e-01,\n",
       "           6.3686e-01, -6.3663e-02],\n",
       "         [-8.5558e-01, -3.8686e-01,  7.3812e-01,  ..., -8.1025e-01,\n",
       "          -5.8888e-01, -1.1894e-01],\n",
       "         ...,\n",
       "         [-3.8833e-01, -4.2675e-01,  5.3241e-01,  ...,  1.7986e-01,\n",
       "           4.1940e-01,  5.0562e-01],\n",
       "         [-1.0859e+00,  9.4982e-01, -5.6402e-01,  ..., -6.5520e-01,\n",
       "           3.1616e-01, -5.1897e-01],\n",
       "         [-1.4108e+00, -5.7014e-01, -1.4222e-01,  ...,  1.2325e+00,\n",
       "          -6.9784e-02, -8.0210e-01]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t,axis=-1) ## Here we do the operation in last axis  (mean value of red , blue , green )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495a7e3",
   "metadata": {},
   "source": [
    "Take only the maximum color channel values (and get the corresponding indices):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d88d47",
   "metadata": {},
   "source": [
    "* This is done all the time in image segmentation models (i.e. take an image, decide which pixels correspond to, say, a car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6374dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.max(t,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d138148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 2,  ..., 2, 0, 1],\n",
       "         [0, 2, 0,  ..., 2, 2, 2],\n",
       "         [2, 1, 0,  ..., 1, 2, 1],\n",
       "         ...,\n",
       "         [1, 1, 2,  ..., 1, 1, 2],\n",
       "         [1, 2, 2,  ..., 0, 1, 0],\n",
       "         [1, 2, 2,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 2,  ..., 2, 2, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 1],\n",
       "         [2, 1, 2,  ..., 1, 1, 2],\n",
       "         ...,\n",
       "         [0, 0, 1,  ..., 2, 0, 2],\n",
       "         [0, 2, 0,  ..., 1, 2, 2],\n",
       "         [0, 2, 0,  ..., 2, 2, 2]],\n",
       "\n",
       "        [[2, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 1, 2,  ..., 1, 1, 1],\n",
       "         [0, 1, 0,  ..., 1, 0, 1],\n",
       "         ...,\n",
       "         [2, 0, 0,  ..., 2, 1, 1],\n",
       "         [1, 1, 1,  ..., 2, 2, 0],\n",
       "         [1, 0, 1,  ..., 0, 1, 1]],\n",
       "\n",
       "        [[1, 1, 2,  ..., 1, 1, 0],\n",
       "         [2, 1, 1,  ..., 2, 0, 1],\n",
       "         [1, 0, 1,  ..., 0, 1, 2],\n",
       "         ...,\n",
       "         [2, 1, 1,  ..., 1, 2, 0],\n",
       "         [0, 0, 1,  ..., 1, 2, 1],\n",
       "         [0, 0, 0,  ..., 1, 0, 2]],\n",
       "\n",
       "        [[1, 2, 0,  ..., 1, 1, 2],\n",
       "         [1, 1, 0,  ..., 2, 2, 1],\n",
       "         [1, 0, 2,  ..., 0, 0, 2],\n",
       "         ...,\n",
       "         [1, 0, 0,  ..., 0, 0, 2],\n",
       "         [0, 0, 1,  ..., 2, 0, 0],\n",
       "         [2, 2, 2,  ..., 1, 0, 0]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices ## it gives the what specific color is maximum in that location "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb118f5",
   "metadata": {},
   "source": [
    "# So Where Do Pytorch and Numpy Differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabba4a9",
   "metadata": {},
   "source": [
    "**Pytorch** starts to really differ from **numpy** in terms of automatically computing gradients of operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6892faa",
   "metadata": {},
   "source": [
    "$$y = \\sum_ix_i^3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f2b54",
   "metadata": {},
   "source": [
    "has a gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ef7ac",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial y}{\\partial x_i} = 3x_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0747cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[5.,8.],[4.,6.]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a44db2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(917., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[5.,8.],[4.,6.]], requires_grad=True)\n",
    "y = x.pow(3).sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f477d51",
   "metadata": {},
   "source": [
    "Compute the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31df7c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75., 192.],\n",
       "        [ 48., 108.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() #compute the gradient\n",
    "x.grad #print the gradient (everything that has happened to x) (everything stored in x !!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15ded2",
   "metadata": {},
   "source": [
    "Double check using the analytical formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5e0e836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75., 192.],\n",
       "        [ 48., 108.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c40ac9",
   "metadata": {},
   "source": [
    "The automatic computation of gradients is the backbone of training deep learning models. Unlike in the example above, most gradient computations don't have an analytical formula, so the automatic computation of gradients is essential. In general, if one has"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e2846",
   "metadata": {},
   "source": [
    "$$y = f(\\overrightarrow{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092fd2ea",
   "metadata": {},
   "source": [
    "Then pytorch can compute $\\partial y/\\partial x_i$ For each of element of the vector $\\vec{x}$ In the context of machine learning,   contains all the weights (also known as parameters) of the neural network and  is the y **Loss Function** of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54151514",
   "metadata": {},
   "source": [
    "# Additional Benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9606013c",
   "metadata": {},
   "source": [
    "In addition, any sort of large matrix multiplication problem is faster with torch tensors than it is with numpy arrays, especially if you're running on a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06106ea5",
   "metadata": {},
   "source": [
    "Using torch: (with a CPU. With GPU, this is much much faster (40 times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45446183",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn((1000,1000))\n",
    "B = torch.randn((1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa4fdc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034500700001444784\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "torch.matmul(A,B)\n",
    "t2 = time.perf_counter()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d2541",
   "metadata": {},
   "source": [
    "Using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d5e2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(int(1e6)).reshape((1000,1000))\n",
    "B = np.random.randn(int(1e6)).reshape((1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "167bb9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11953910000011092\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "A@B\n",
    "t2 = time.perf_counter()\n",
    "print(t2-t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
