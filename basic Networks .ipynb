{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0457e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.optim import SGD\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3bf714",
   "metadata": {},
   "source": [
    "Suppose one has data that consists of an independent vector and a dependent vector $x_i$ and $y_i$ (i is the i'th value  in the data set ) .For example :\n",
    "\n",
    "   * $x_i$ is the height of the th person, and  is their weight (predict weight using height)\n",
    "   * $x_i$ is a picture of a handwritten digit, and  is the digit itself (predict numbers from hand written numbers)\n",
    "   * $x_i$ is a CT scan of a patient, and  are the corresponding pixels corresponding to tumours (my research)\n",
    "   \n",
    "The goal of a neural network is as follows. Define a function $f$  that depends on parameters  that makes predictions   \n",
    "                                                $$\\hat{y_i} = f(x_i;a)$$\n",
    "One wants to make  $y_i$ (the predictions) and $y_i$ (the true values) as close as possible by modifying the values of a. What does as close as possible mean? This depends on the task. In general, one defines a similarity function (or Loss function) $L(y,\\hat{y})$. The more similar all the $y_i$s and $\\hat{y_i}$s are, the smaller  should be. For example , this could be as simple as                                                \n",
    "$$L(y,\\hat{y}) = \\sum_i(y_i-\\hat{y_i})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf34f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()\n",
    "y = torch.tensor([1,5,2,5]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b5a26ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 2.],\n",
       "        [5., 2.],\n",
       "        [1., 3.],\n",
       "        [7., 6.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8802c9",
   "metadata": {},
   "source": [
    "*  $x_1 = (6,2),x_2=(5,2)$\n",
    "*  $y_1 = 1,y_2 = 5$\n",
    "\n",
    "We want to find a function $f$ that depends on parameters $a$ that lets us get from $x$ to $y$.\n",
    "\n",
    "**Idea**\n",
    "\n",
    "   1. First multiply each element in x by a $8 \\times 2$  matrix (this is 16 parameters )\n",
    "   2. Then multiply each element in x by a $1\\times 8$ matrix (this is 8 parameters )\n",
    "   \n",
    "Define a matrix (takes in a 2d vector and returns a 8d vector).   \n",
    "* **IMPORTANT: When the matrix is created, it is initially created with random values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "291c1aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=8, bias=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = nn.Linear(2,8,bias=False)  ## Not taking bias here i.e b is not here \n",
    "M1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1057bad8",
   "metadata": {},
   "source": [
    "If one passes in a vector $x$ (the dataset) where each element $x_i$ (an instance) is a 2d vector, $M$ will apply the same matrix multiplication to each element $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "053039f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0580, -0.7191,  3.2259, -3.3918, -0.1126,  1.7774, -0.2962, -2.0141],\n",
       "        [-2.5387, -0.4994,  2.7067, -3.0371,  0.1139,  1.2807, -0.1369, -1.4844],\n",
       "        [-0.4325,  0.6788,  0.6851, -2.2503,  1.6430, -1.3078,  0.8296,  1.2166],\n",
       "        [-3.4615,  0.2592,  3.9663, -6.2740,  2.1535, -0.1318,  0.8631, -0.2154]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db7fcd",
   "metadata": {},
   "source": [
    "We can chain this with a second matrix  **M2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04305de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=1, bias=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2 = nn.Linear(8,1, bias=False)\n",
    "M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "316c48df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4823],\n",
       "        [1.3598],\n",
       "        [1.2439],\n",
       "        [3.1000]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2(M1(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485ef78",
   "metadata": {},
   "source": [
    "M2(M1(x)) have extra dimension hence we apply squeeze() to get same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da583f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4823, 1.3598, 1.2439, 3.1000], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2(M1(x)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e3739a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 2., 5.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f22e72",
   "metadata": {},
   "source": [
    "The weights of the matrices M1 and M2 consitute the weights **a**  of the network defined above. In order to optimize for these weights, we first construct our network **f** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec926938",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2,8,bias=False)\n",
    "        self.Matrix2 = nn.Linear(8,1,bias=False)\n",
    "    def forward(self,x):\n",
    "        x = self.Matrix1(x)\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa6c9c0",
   "metadata": {},
   "source": [
    "Constructing the network using a subclass of the nn.Module allows the parameters of the network to be conveniently stored. This will be useful later when we need to adjust them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da60cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = MyNeuralNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041ace1",
   "metadata": {},
   "source": [
    "Pass in data to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "076a8d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.1668, -1.8191, -0.4687, -2.6758], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = f(x)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be5679ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 2., 5.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d61d962",
   "metadata": {},
   "source": [
    "## Now we are  adjusting $a$ so that $\\hat{y}$ and $y$ are similar\n",
    "\n",
    "Now we define the loss function $L$ , which provides a metric of similarity between $y$ and $\\hat{y}$\n",
    ". In this case, we will use the mean squared error loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "058c6342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30.3854, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = nn.MSELoss()\n",
    "L(y,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9f308",
   "metadata": {},
   "source": [
    "Confirming it is doing the same as the regular mean-squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10b5b5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30.3854, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((y-yhat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeccb236",
   "metadata": {},
   "source": [
    "Note that $L$ depends on $a$, since our predictions $\\hat{y}$\n",
    " depend on the parameters of the network a . In this sense, $L=L(a)$ . The main idea behind machine learning is to compute\n",
    " \n",
    " $$\\frac{\\partial L}{\\partial a_i}$$\n",
    " \n",
    " for each parameter $a_i$ of the network . Then we adjust each parameter as follows \n",
    " \n",
    " $$a_i \\to a_i - l\\frac{\\partial L}{\\partial a_i}$$\n",
    " where $l$ is the learning rate .\n",
    " \n",
    " The idea is to do this over and over again, until one reaches a minimum for $L$. This is called gradient descent.\n",
    " \n",
    " * Each pass of the full data set $x$ is called an epoch. In this case, we are evaluating $\\frac{\\partial L}{\\partial a_i}$ on the entire dataset $x$ each time we iterate $a_i \\to a_i - l\\frac{\\partial L}{\\partial a_i}$ , so each iteration corresponds to an epoch.\n",
    " \n",
    "The SGD(meaning stochastic gradient descent) takes in all model parameters $a$ along with the learning rate $l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0d5f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(f.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03508755",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for _ in range(50):\n",
    "    opt.zero_grad() # flush previous epoch's gradient\n",
    "    loss_value = L(f(x), y) #compute loss\n",
    "    loss_value.backward() # compute gradient ; computes dloss/dx for every parameter x which has requires_grad=True\n",
    "    opt.step() # Perform iteration using gradient above ; this thing updates the value of x using the gradient x.grad. For example, the SGD optimizer performs: x += -lr * x.grad\n",
    "    losses.append(loss_value.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f437c240",
   "metadata": {},
   "source": [
    "**Explaination:** loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x. In pseudo-code:\n",
    "\n",
    "x.grad += dloss/dx\n",
    "optimizer.step updates the value of x using the gradient x.grad. For example, the SGD optimizer performs:\n",
    "\n",
    "x += -lr * x.grad\n",
    "optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.\n",
    "\n",
    "If you have multiple losses (loss1, loss2) you can sum them and then call backwards once:\n",
    "\n",
    "loss3 = loss1 + loss2\n",
    "loss3.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d14832b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epochs')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylabel('Loss $L(y,\\hat{y};a)$')\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac4325",
   "metadata": {},
   "source": [
    "This is as close as we can make the model  predict  from :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79896896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8634, 2.5563, 1.8384, 5.2122], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "737a0141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 2., 5.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604976fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
